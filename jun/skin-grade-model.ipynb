{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\human\\.conda\\envs\\human-dl-env2\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\human\\.conda\\envs\\human-dl-env2\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\human\\.conda\\envs\\human-dl-env2\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\human\\.conda\\envs\\human-dl-env2\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\human\\.conda\\envs\\human-dl-env2\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\human\\.conda\\envs\\human-dl-env2\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "22/22 [==============================] - 7s 277ms/step - loss: 1.3525 - accuracy: 0.4854 - val_loss: 1.2416 - val_accuracy: 0.5174\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 6s 259ms/step - loss: 1.2186 - accuracy: 0.5160 - val_loss: 1.1789 - val_accuracy: 0.5174\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 6s 268ms/step - loss: 1.2232 - accuracy: 0.4942 - val_loss: 1.4304 - val_accuracy: 0.3081\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 6s 255ms/step - loss: 1.2885 - accuracy: 0.4883 - val_loss: 1.1764 - val_accuracy: 0.5174\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 6s 254ms/step - loss: 1.1036 - accuracy: 0.5204 - val_loss: 1.2133 - val_accuracy: 0.4012\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 5s 244ms/step - loss: 1.0984 - accuracy: 0.5233 - val_loss: 1.2706 - val_accuracy: 0.5407\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 5s 234ms/step - loss: 1.1371 - accuracy: 0.5496 - val_loss: 1.1151 - val_accuracy: 0.5407\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 6s 291ms/step - loss: 1.1285 - accuracy: 0.5350 - val_loss: 1.1900 - val_accuracy: 0.5174\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 6s 293ms/step - loss: 1.1593 - accuracy: 0.5262 - val_loss: 1.1214 - val_accuracy: 0.5291\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 6s 253ms/step - loss: 1.0792 - accuracy: 0.5350 - val_loss: 1.1405 - val_accuracy: 0.4942\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 1.1405 - accuracy: 0.4942\n",
      "테스트 정확도: 0.49418604373931885\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 이미지와 레이블 데이터를 저장할 리스트\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# 데이터 디렉토리 설정\n",
    "cropped_image_dir = r\"D:\\data\\korean 01 data\"\n",
    "json_dir = r\"D:\\data\\korean skin data\\open data\\data\\Training\\Label data\\digiter cam\"\n",
    "\n",
    "# 데이터 로드\n",
    "for id_folder in os.listdir(cropped_image_dir):\n",
    "    image_folder_path = os.path.join(cropped_image_dir, id_folder)\n",
    "    json_file_path = os.path.join(json_dir, id_folder, f\"{id_folder}_01_F_01.json\")\n",
    "\n",
    "    if not os.path.exists(json_file_path):\n",
    "        continue\n",
    "\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "        json_data = json.load(f)\n",
    "    \n",
    "    for image_name in os.listdir(image_folder_path):\n",
    "        image_path = os.path.join(image_folder_path, image_name)\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.resize(image, (128, 128))  # 예시로 128x128로 크기 조정\n",
    "        images.append(image)\n",
    "\n",
    "        # 레이블 추출 (예: 이마의 색소침착)\n",
    "        label = json_data['annotations']['forehead_pigmentation']\n",
    "        labels.append(label)\n",
    "\n",
    "# 이미지와 레이블을 NumPy 배열로 변환\n",
    "images = np.array(images, dtype='float32') / 255.0  # 정규화\n",
    "labels = np.array(labels)\n",
    "\n",
    "# 레이블 원-핫 인코딩\n",
    "labels = to_categorical(labels, num_classes=6)  \n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# 모델 설계\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(6, activation='softmax')\n",
    "])\n",
    "\n",
    "# 모델 컴파일\n",
    "\n",
    "model.compile(optimizer= 'adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# 모델 평가\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"테스트 정확도: {test_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "레이블 값들: [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# 레이블 값 확인\n",
    "unique_labels = np.unique(labels)\n",
    "print(f\"레이블 값들: {unique_labels}\")\n",
    "\n",
    "# 최대 레이블 값에 따라 num_classes 설정\n",
    "num_classes = len(unique_labels)  # 레이블 값 개수를 num_classes로 설정\n",
    "\n",
    "# 레이블 원-핫 인코딩\n",
    "labels = to_categorical(labels, num_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_image_dir = r\"D:\\data\\korean 01 data\"\n",
    "json_dir = r\"D:\\data\\korean skin data\\open data\\data\\Training\\Label data\\digiter cam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 로드\n",
    "def load_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        return None\n",
    "    img = cv2.resize(img , (224,224))\n",
    "    img = img / 255.0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json 파일 로드\n",
    "def load_json(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    age = data['info']['age']\n",
    "    gender = 1 if data['info']['gender'] =='F' else 0 \n",
    "    skin_type = data['info']['skin_type']\n",
    "    sensitive = data['info']['sensitive']\n",
    "\n",
    "    pigmentation = data['annotations']['forehead_pigmentation']\n",
    "    \n",
    "    return age, gender, skin_type, sensitive, pigmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "image_dir = r\"D:\\data\\korean 01 data\"\n",
    "json_dir = r\"D:\\data\\korean skin data\\open data\\data\\Training\\Label data\\digiter cam\"\n",
    "\n",
    "images = []\n",
    "metadata = []\n",
    "\n",
    "for folder_name in os.listdir(image_dir):\n",
    "    folder_path = os.path.join(image_dir, folder_name)\n",
    "    if os.path.isdir(folder_path):  # 폴더인지 확인\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith('.jpg'):  # .jpg 파일만 처리\n",
    "                image_path = os.path.join(folder_path, filename)\n",
    "                \n",
    "                # JSON 파일명 생성: 'cropped_' 제거하고 '.jpg' -> '.json'\n",
    "                json_filename = filename.replace('cropped_', '').replace('.jpg', '')\n",
    "                json_path = os.path.join(json_dir, folder_name, json_filename)  # JSON 파일 경로\n",
    "                \n",
    "                # 이미지와 JSON 로드\n",
    "                image = load_image(image_path)\n",
    "                if image is None:\n",
    "                    continue  # 이미지 로드 실패시 건너뛰기\n",
    "                age, gender, skin_type, sensitive, pigmentation = load_json(json_path)\n",
    "                \n",
    "                images.append(image)  # images 리스트에 추가\n",
    "                metadata.append([age, gender, skin_type, sensitive, pigmentation])  # metadata 리스트에 추가\n",
    "\n",
    "# 리스트를 numpy 배열로 변환\n",
    "images = np.array(images)\n",
    "metadata = np.array(metadata)\n",
    "\n",
    "X = images\n",
    "y = metadata[:,-1] # target\n",
    "\n",
    "X_metadata = metadata[:, :-1] # pigment 제외한 나머지 데이터\n",
    "\n",
    "\n",
    "X_train,X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
    "\n",
    "X_train_images = X_train\n",
    "X_val_images = X_val\n",
    "\n",
    "X_train_metadata = X_metadata[:len(X_train)]\n",
    "X_val_metadata = X_metadata[len(X_train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " image_input (InputLayer)    [(None, 224, 224, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)          (None, 222, 222, 32)         896       ['image_input[0][0]']         \n",
      "                                                                                                  \n",
      " max_pooling2d_15 (MaxPooli  (None, 111, 111, 32)         0         ['conv2d_16[0][0]']           \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)          (None, 109, 109, 64)         18496     ['max_pooling2d_15[0][0]']    \n",
      "                                                                                                  \n",
      " max_pooling2d_16 (MaxPooli  (None, 54, 54, 64)           0         ['conv2d_17[0][0]']           \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " metadata_input (InputLayer  [(None, 4)]                  0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " flatten_7 (Flatten)         (None, 186624)               0         ['max_pooling2d_16[0][0]']    \n",
      "                                                                                                  \n",
      " dense_26 (Dense)            (None, 32)                   160       ['metadata_input[0][0]']      \n",
      "                                                                                                  \n",
      " dense_25 (Dense)            (None, 64)                   1194400   ['flatten_7[0][0]']           \n",
      "                                                          0                                       \n",
      "                                                                                                  \n",
      " dense_27 (Dense)            (None, 16)                   528       ['dense_26[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate  (None, 80)                   0         ['dense_25[0][0]',            \n",
      " )                                                                   'dense_27[0][0]']            \n",
      "                                                                                                  \n",
      " dense_28 (Dense)            (None, 6)                    486       ['concatenate_6[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11964566 (45.64 MB)\n",
      "Trainable params: 11964566 (45.64 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input\n",
    "\n",
    "\n",
    "image_input = Input(shape=(224,224,3), name= 'image_input')\n",
    "x = layers.Conv2D(32,(3,3), activation = 'relu')(image_input)\n",
    "x = layers.MaxPooling2D((2,2))(x)\n",
    "x = layers.Conv2D(64,(3,3), activation = 'relu')(x)\n",
    "x = layers.MaxPooling2D((2,2))(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "\n",
    "metadata_input = Input(shape=(4,), name='metadata_input')\n",
    "y = layers.Dense(32, activation ='relu')(metadata_input)\n",
    "y = layers.Dense(16, activation='relu')(y)\n",
    "\n",
    "combined = layers.concatenate([x,y])\n",
    "output = layers.Dense(6, activation='softmax')(combined)\n",
    "\n",
    "model = models.Model(inputs =[image_input, metadata_input], outputs=output)\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "22/22 [==============================] - 16s 675ms/step - loss: 3.1633 - accuracy: 0.4082 - val_loss: 1.4000 - val_accuracy: 0.3721\n",
      "Epoch 2/20\n",
      "22/22 [==============================] - 14s 641ms/step - loss: 1.2670 - accuracy: 0.4840 - val_loss: 1.2315 - val_accuracy: 0.5174\n",
      "Epoch 3/20\n",
      "22/22 [==============================] - 14s 627ms/step - loss: 1.2156 - accuracy: 0.5044 - val_loss: 1.1834 - val_accuracy: 0.5174\n",
      "Epoch 4/20\n",
      "22/22 [==============================] - 14s 630ms/step - loss: 1.1590 - accuracy: 0.5117 - val_loss: 1.1690 - val_accuracy: 0.5291\n",
      "Epoch 5/20\n",
      "22/22 [==============================] - 14s 632ms/step - loss: 1.1325 - accuracy: 0.5233 - val_loss: 1.1526 - val_accuracy: 0.5116\n",
      "Epoch 6/20\n",
      "22/22 [==============================] - 14s 628ms/step - loss: 1.1206 - accuracy: 0.5175 - val_loss: 1.1677 - val_accuracy: 0.4884\n",
      "Epoch 7/20\n",
      "22/22 [==============================] - 14s 638ms/step - loss: 1.1092 - accuracy: 0.5015 - val_loss: 1.1567 - val_accuracy: 0.5233\n",
      "Epoch 8/20\n",
      "22/22 [==============================] - 14s 630ms/step - loss: 1.0777 - accuracy: 0.5364 - val_loss: 1.1961 - val_accuracy: 0.5233\n",
      "Epoch 9/20\n",
      "22/22 [==============================] - 14s 624ms/step - loss: 1.1246 - accuracy: 0.5219 - val_loss: 1.1862 - val_accuracy: 0.4709\n",
      "Epoch 10/20\n",
      "22/22 [==============================] - 14s 640ms/step - loss: 1.0966 - accuracy: 0.5029 - val_loss: 1.1174 - val_accuracy: 0.5407\n",
      "Epoch 11/20\n",
      "22/22 [==============================] - 14s 635ms/step - loss: 1.1257 - accuracy: 0.5335 - val_loss: 1.1719 - val_accuracy: 0.4884\n",
      "Epoch 12/20\n",
      "22/22 [==============================] - 14s 627ms/step - loss: 1.0759 - accuracy: 0.5364 - val_loss: 1.1110 - val_accuracy: 0.5407\n",
      "Epoch 13/20\n",
      "22/22 [==============================] - 14s 638ms/step - loss: 1.0274 - accuracy: 0.5685 - val_loss: 1.1223 - val_accuracy: 0.5349\n",
      "Epoch 14/20\n",
      "22/22 [==============================] - 14s 619ms/step - loss: 1.0341 - accuracy: 0.5729 - val_loss: 1.1273 - val_accuracy: 0.5349\n",
      "Epoch 15/20\n",
      "22/22 [==============================] - 15s 697ms/step - loss: 1.0467 - accuracy: 0.5321 - val_loss: 1.1198 - val_accuracy: 0.5349\n",
      "Epoch 16/20\n",
      "22/22 [==============================] - 14s 638ms/step - loss: 1.0399 - accuracy: 0.5539 - val_loss: 1.1371 - val_accuracy: 0.5349\n",
      "Epoch 17/20\n",
      "22/22 [==============================] - 14s 623ms/step - loss: 1.0187 - accuracy: 0.5700 - val_loss: 1.1245 - val_accuracy: 0.5407\n",
      "Epoch 18/20\n",
      "22/22 [==============================] - 14s 627ms/step - loss: 1.0379 - accuracy: 0.5612 - val_loss: 1.1183 - val_accuracy: 0.5291\n",
      "Epoch 19/20\n",
      "22/22 [==============================] - 16s 715ms/step - loss: 1.0441 - accuracy: 0.5656 - val_loss: 1.1172 - val_accuracy: 0.5581\n",
      "Epoch 20/20\n",
      "22/22 [==============================] - 14s 617ms/step - loss: 1.0149 - accuracy: 0.5787 - val_loss: 1.0900 - val_accuracy: 0.5349\n",
      "Training History:  {'loss': [3.1632609367370605, 1.2670376300811768, 1.215551733970642, 1.1590192317962646, 1.132548213005066, 1.1206104755401611, 1.1091551780700684, 1.0776894092559814, 1.1246044635772705, 1.0966054201126099, 1.1256858110427856, 1.0758707523345947, 1.0273834466934204, 1.034079909324646, 1.0466800928115845, 1.039857268333435, 1.0187398195266724, 1.0378512144088745, 1.044136881828308, 1.0148507356643677], 'accuracy': [0.40816327929496765, 0.4839650094509125, 0.5043731927871704, 0.5116618275642395, 0.5233235955238342, 0.5174927115440369, 0.5014577507972717, 0.5364431738853455, 0.5218659043312073, 0.5029154419898987, 0.533527672290802, 0.5364431738853455, 0.5685130953788757, 0.5728862881660461, 0.532069981098175, 0.5539358854293823, 0.5699708461761475, 0.5612244606018066, 0.565597653388977, 0.5787171721458435], 'val_loss': [1.3999511003494263, 1.2314521074295044, 1.1833622455596924, 1.1690119504928589, 1.1526316404342651, 1.1677396297454834, 1.156676173210144, 1.1961356401443481, 1.1862385272979736, 1.117437481880188, 1.1718666553497314, 1.1109877824783325, 1.1222649812698364, 1.127292513847351, 1.1198155879974365, 1.1371334791183472, 1.1245077848434448, 1.118325114250183, 1.1171555519104004, 1.0899670124053955], 'val_accuracy': [0.3720930218696594, 0.5174418687820435, 0.5174418687820435, 0.5290697813034058, 0.5116279125213623, 0.4883720874786377, 0.5232558250427246, 0.5232558250427246, 0.47093021869659424, 0.5406976938247681, 0.4883720874786377, 0.5406976938247681, 0.5348837375640869, 0.5348837375640869, 0.5348837375640869, 0.5348837375640869, 0.5406976938247681, 0.5290697813034058, 0.5581395626068115, 0.5348837375640869]}\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "history = model.fit(\n",
    "    {'image_input': X_train_images, 'metadata_input': X_train_metadata},  # 학습 데이터\n",
    "    y_train,  # 실제 레이블 (pigmentation 값)\n",
    "    epochs=20,  # 에포크 수\n",
    "    validation_data=(\n",
    "        {'image_input': X_val_images, 'metadata_input': X_val_metadata},  # 검증 데이터\n",
    "        y_val  # 검증 데이터의 실제 레이블 (pigmentation 값)\n",
    "    )\n",
    ")\n",
    "\n",
    "# 학습이 완료되면 학습 과정에서의 손실과 메트릭스를 확인할 수 있습니다.\n",
    "print(\"Training History: \", history.history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 112ms/step - loss: 1.0900 - accuracy: 0.5349\n",
      "Test Loss:  1.0899670124053955\n",
      "Test Accuracy:  0.5348837375640869\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가\n",
    "test_loss, test_accuracy = model.evaluate(\n",
    "    {'image_input': X_val_images, 'metadata_input': X_val_metadata},  # 검증 데이터\n",
    "    y_val  # 검증 데이터의 실제 레이블\n",
    ")\n",
    "\n",
    "print(\"Test Loss: \", test_loss)\n",
    "print(\"Test Accuracy: \", test_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "human-dl-env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
